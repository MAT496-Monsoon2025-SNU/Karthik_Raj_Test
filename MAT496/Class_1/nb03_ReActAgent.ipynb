{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9541200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a710feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def unaware() -> str:\n",
    "    \"\"\" Call this tool if user says something that you don't know the answer to\"\"\"\n",
    "    return \"I am unaware\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def too_short() -> str:\n",
    "    \"\"\" Call this too if the user input is too short and is not meaningful\"\"\"\n",
    "    return \"Type more lazy bones!\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def correct() -> str:\n",
    "    \"\"\" Call this tool if you want to correct the user\"\"\"\n",
    "    return \"You are not correct\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8945c71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_list = [unaware, too_short, correct ]\n",
    "tools_dict = {t.name: t for t in tools_list} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "845945a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\n",
      "  Downloading langgraph-0.6.7-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: langchain-core>=0.1 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langgraph) (0.3.75)\n",
      "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
      "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
      "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
      "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langgraph) (2.11.7)\n",
      "Collecting xxhash>=3.5.0 (from langgraph)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
      "  Downloading ormsgpack-1.10.0-cp39-cp39-win_amd64.whl.metadata (44 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core>=0.1->langgraph) (0.4.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core>=0.1->langgraph) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core>=0.1->langgraph) (4.14.1)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hp\\anaconda3\\envs\\langchain\\lib\\site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
      "Downloading langgraph-0.6.7-py3-none-any.whl (153 kB)\n",
      "Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
      "Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "Downloading ormsgpack-1.10.0-cp39-cp39-win_amd64.whl (121 kB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
      "\n",
      "   ------ --------------------------------- 1/6 [ormsgpack]\n",
      "   ------------- -------------------------- 2/6 [langgraph-sdk]\n",
      "   ------------- -------------------------- 2/6 [langgraph-sdk]\n",
      "   -------------------- ------------------- 3/6 [langgraph-checkpoint]\n",
      "   -------------------------- ------------- 4/6 [langgraph-prebuilt]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   --------------------------------- ------ 5/6 [langgraph]\n",
      "   ---------------------------------------- 6/6 [langgraph]\n",
      "\n",
      "Successfully installed langgraph-0.6.7 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.9 ormsgpack-1.10.0 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6cbdcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "agent = create_react_agent(\"groq:openai/gpt-oss-120b\", tools_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a024d984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='The Sun goes around the Earth.', additional_kwargs={}, response_metadata={}, id='1bcf98da-6691-4401-aa11-6612adcaa470'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'The user statement is false. We need to correct. Use the correct tool.', 'tool_calls': [{'id': 'fc_305de0e0-0772-4c6d-a8d1-85f296194c4b', 'function': {'arguments': '{\"correction\":\"The Sun does not go around the Earth; the Earth orbits the Sun.\"}', 'name': 'correct'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 58, 'prompt_tokens': 177, 'total_tokens': 235, 'completion_time': 0.118242812, 'prompt_time': 0.007438161, 'queue_time': 0.060160579, 'total_time': 0.125680973}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_3a688838c3', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--19484969-bb14-408d-88b8-b25867951413-0', tool_calls=[{'name': 'correct', 'args': {'correction': 'The Sun does not go around the Earth; the Earth orbits the Sun.'}, 'id': 'fc_305de0e0-0772-4c6d-a8d1-85f296194c4b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 177, 'output_tokens': 58, 'total_tokens': 235}),\n",
       "  ToolMessage(content='You are not correct', name='correct', id='b450b067-d1af-4ca6-80b0-9542a8ea1cba', tool_call_id='fc_305de0e0-0772-4c6d-a8d1-85f296194c4b'),\n",
       "  AIMessage(content='Your statement is scientifically inaccurate. In the heliocentric model, which is supported by centuries of observation and modern physics, the Earth **orbits** the Sun, not the other way around. Here are a few key pieces of evidence:\\n\\n1. **Stellar Parallax** – As Earth moves around the Sun, nearby stars appear to shift slightly against the background of distant stars. This effect was first measured in the 19th\\u202fcentury and can only be explained if Earth is moving around the Sun.\\n\\n2. **Phases of Venus** – Observations by Galileo in 1610 showed that Venus exhibits a full set of phases (new, crescent, gibbous, full) just like the Moon. These phases make sense only if Venus orbits the Sun inside Earth’s orbit, which requires a Sun‑centered system.\\n\\n3. **Aberration of Starlight** – The apparent direction of incoming starlight changes slightly over the year because of Earth’s orbital velocity. This phenomenon was discovered by James Bradley in 1728 and is a direct consequence of Earth’s motion around the Sun.\\n\\n4. **Newton’s Law of Gravitation** – The gravitational force between two masses causes them to orbit their common center of mass. Because the Sun is ~330,000 times more massive than Earth, the center of mass lies very close to the Sun’s center, making the Sun’s motion around Earth negligible.\\n\\n5. **Spacecraft Navigation** – Modern interplanetary missions (e.g., Voyager, Mars rovers, New Horizons) are plotted using heliocentric trajectories. Their successful arrivals confirm that the Sun is the central gravitational anchor of the Solar System.\\n\\nSo, the scientifically accepted model is that **the Earth revolves around the Sun** once every ~365.25\\u202fdays, while the Sun itself moves only slightly around the Solar System’s barycenter (the common center of mass of all planets, moons, and the Sun).', additional_kwargs={'reasoning_content': 'The user says \"The Sun goes around the Earth.\" The assistant responded with correction, but the system says \"You are not correct\". Possibly the user is claiming that statement is true? The assistant must correct them. The user says \"The Sun goes around the Earth.\" The assistant responded with correction \"The Sun does not go around the Earth; the Earth orbits the Sun.\" Then the system says \"You are not correct\". That seems contradictory. Actually the user might be testing. The correct scientific fact: Earth orbits the Sun. So the assistant\\'s correction is correct. The system says \"You are not correct\" perhaps as a prompt to re-evaluate? The instruction: we must correct the user if they say something false. The user statement is false, we must correct. The system says \"You are not correct\" maybe indicating the assistant\\'s correction was not correct? But it\\'s correct. Could be a trick: maybe the user is referencing a geocentric model historically? But the correct answer is that the Sun does not go around Earth. So we should stand by correction. Perhaps we need to provide more explanation, evidence. So respond with a more detailed explanation, citing heliocentric model, observational evidence, etc. Also be polite.'}, response_metadata={'token_usage': {'completion_tokens': 657, 'prompt_tokens': 221, 'total_tokens': 878, 'completion_time': 1.376904607, 'prompt_time': 0.009968241, 'queue_time': 0.173937769, 'total_time': 1.3868728479999999}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_3a688838c3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e03d139c-12fc-4ea0-9507-01864ba70d60-0', usage_metadata={'input_tokens': 221, 'output_tokens': 657, 'total_tokens': 878})]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"messages\": \"The Sun goes around the Earth.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7bdf9cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is the price of an apple in my neighbourhood store?', additional_kwargs={}, response_metadata={}, id='c14e5ec8-7112-4a82-935f-6a88a35a50c1'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'The user asks: \"What is the price of an apple in my neighbourhood store?\" This is specific local info that the assistant doesn\\'t have. We should respond that we don\\'t have that info and suggest checking local store or online. According to guidelines, we can say we don\\'t know. Possibly use the unaware tool? The \"unaware\" tool is for when we don\\'t know the answer. So we should call that.', 'tool_calls': [{'id': 'fc_fc2c9287-7b60-4db7-83fa-1e0bb6a88d58', 'function': {'arguments': '{}', 'name': 'unaware'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 103, 'prompt_tokens': 182, 'total_tokens': 285, 'completion_time': 0.215331827, 'prompt_time': 0.006531735, 'queue_time': 0.046914445, 'total_time': 0.221863562}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_213abb2467', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d83d87f0-e2e2-4fe3-8bc7-b7ef89751a3d-0', tool_calls=[{'name': 'unaware', 'args': {}, 'id': 'fc_fc2c9287-7b60-4db7-83fa-1e0bb6a88d58', 'type': 'tool_call'}], usage_metadata={'input_tokens': 182, 'output_tokens': 103, 'total_tokens': 285}),\n",
       "  ToolMessage(content='I am unaware', name='unaware', id='20689bde-d930-4309-950c-b4ef00c417f0', tool_call_id='fc_fc2c9287-7b60-4db7-83fa-1e0bb6a88d58'),\n",
       "  AIMessage(content='I’m afraid I don’t have access to real‑time or location‑specific pricing information, so I can’t tell you the current price of an apple at your neighbourhood store. If you’d like, you could check the store’s website, give them a quick call, or stop by in person to get the most accurate price. Let me know if there’s anything else I can help with!', additional_kwargs={'reasoning_content': 'The user asks: \"What is the price of an apple in my neighbourhood store?\" The assistant does not have that information; we need to respond that we don\\'t have that data. According to policy, we should say we don\\'t have access to real-time or personal location data. So we should politely say we can\\'t know that.'}, response_metadata={'token_usage': {'completion_tokens': 155, 'prompt_tokens': 206, 'total_tokens': 361, 'completion_time': 0.315502748, 'prompt_time': 0.010280421, 'queue_time': 0.119175038, 'total_time': 0.325783169}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_213abb2467', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3a22c662-1d22-4d04-8d89-f4a25a3d286a-0', usage_metadata={'input_tokens': 206, 'output_tokens': 155, 'total_tokens': 361})]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"messages\": \"What is the price of an apple in my neighbourhood store?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef3b10a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Are you aware of sql?', additional_kwargs={}, response_metadata={}, id='59dfffed-bb77-403d-943b-52db885b9511'),\n",
       "  AIMessage(content='Yes—I’m familiar with SQL (Structured Query Language). I can help you write queries, design schemas, troubleshoot errors, optimize performance, and answer many other SQL‑related questions. Let me know what you need!', additional_kwargs={'reasoning_content': 'The user asks: \"Are you aware of sql?\" It\\'s a simple question. I can answer yes, I am aware of SQL, can help with queries, etc. No tool needed.'}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 176, 'total_tokens': 267, 'completion_time': 0.186636829, 'prompt_time': 0.007294161, 'queue_time': 0.053623309, 'total_time': 0.19393099}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_ed9190d8b7', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b63bbeac-c579-4e01-b30d-96d158668b9e-0', usage_metadata={'input_tokens': 176, 'output_tokens': 91, 'total_tokens': 267})]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"messages\": \"Are you aware of sql?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "880787c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Life', additional_kwargs={}, response_metadata={}, id='249eefa8-54ab-4369-aa90-1a739e283382'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'The user wrote \"Life\". Likely they want a philosophical answer or something about meaning of life. We should respond with thoughtful reflection. It\\'s a vague prompt; maybe ask clarifying question? Could give a short philosophical reflection. Since it\\'s not too short? It\\'s a single word, maybe considered too short. According to tool, \"too_short\" if input is too short and not meaningful. Probably we should ask for clarification. But we can respond with a philosophical answer. Usually we can respond with something like \"What about life?\" But guidelines: if too short, we can ask for clarification. Use tool too_short? The user gave \"Life\". That is a one-word prompt, ambiguous. The instruction: \"Call this tool if the user input is too short and is not meaningful\". Probably we should call too_short. Let\\'s do that.', 'tool_calls': [{'id': 'fc_10a4a28e-3f08-4e04-aa2c-5b4aa9f2aa68', 'function': {'arguments': '{}', 'name': 'too_short'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 190, 'prompt_tokens': 171, 'total_tokens': 361, 'completion_time': 0.394237435, 'prompt_time': 0.006516433, 'queue_time': 0.052866477, 'total_time': 0.400753868}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_ed9190d8b7', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f96acf0e-fd79-4fcd-86a2-0b337773d6aa-0', tool_calls=[{'name': 'too_short', 'args': {}, 'id': 'fc_10a4a28e-3f08-4e04-aa2c-5b4aa9f2aa68', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 190, 'total_tokens': 361}),\n",
       "  ToolMessage(content='Type more lazy bones!', name='too_short', id='aa03f712-acf3-42a5-b962-0f88b92154da', tool_call_id='fc_10a4a28e-3f08-4e04-aa2c-5b4aa9f2aa68'),\n",
       "  AIMessage(content='Life can mean a lot of different things depending on the context—philosophical musings, personal experiences, scientific facts, or simply a casual “how’s it going?”\\u202fWhat aspect of life are you interested in exploring? Are you looking for a deep philosophical discussion, practical advice, a fun fact, or something else entirely? Let me know, and we can dive right in!', additional_kwargs={'reasoning_content': 'User wrote \"Life\". Probably they want a philosophical discussion or something. Need to respond appropriately, maybe ask for clarification or elaborate. Not too short. So we can ask them what they want: meaning of life, advice, etc. We should respond with an open-ended question.'}, response_metadata={'token_usage': {'completion_tokens': 145, 'prompt_tokens': 198, 'total_tokens': 343, 'completion_time': 0.290851146, 'prompt_time': 0.007859534, 'queue_time': 0.052334896, 'total_time': 0.29871068}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_3a688838c3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b79b5a73-6865-48af-99bc-fa35ffd4ecd3-0', usage_metadata={'input_tokens': 198, 'output_tokens': 145, 'total_tokens': 343})]}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"messages\": \"Life\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aed41b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
